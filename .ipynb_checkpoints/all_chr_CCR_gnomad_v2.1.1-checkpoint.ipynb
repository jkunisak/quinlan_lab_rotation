{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import dill \n",
    "dill.dump_session(\"all_chr_CCR_jason.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.load_session(\"all_chr_CCR_jason.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jason Kunisaki - Quinlan Lab Rotation\n",
    "## Evaluate CCRs\n",
    "\n",
    "This notebook contains the script needed to recapitulate CCRs from the first iteration of CCRs [(see manuscript)](https://www.nature.com/articles/s41588-018-0294-6.epdf?author_access_token=N8RkVYcavtplcSSE2KJkNdRgN0jAjWel9jnR3ZoTv0OG-o3UTB17cpoBs8B6XMBCl-5E0ZvpOii0iPl_hRSMGjWfkG4em7gjy95eV4bkTb0AF-E_dj3obeJfaadTja3aj9hUh1xk_BIztWgJScFx8w%3D%3D) and develop new filter parameters to better define CCRs. These parameters include **VAF, SIFT/PolyPhen scores, and coverage**. Use [PathoScore](https://github.com/quinlan-lab/pathoscore) to determine the efficacy of CCRs as a classifier for clinvar pathogenic variants. SIFT and PolyPhen interpretations can be found in the [VEP annotation descriptions](https://uswest.ensembl.org/info/genome/variation/prediction/protein_function.html). VAF filter is based on the percentile of VAFs across all variants from exome and whole genome sequencing (percentiles are calculated independent of sequencing used).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get necessary files - gnomad version: 2.1.1 \n",
    "```\n",
    "Exome Sequencing Gnomad file: \n",
    "cd ~/git/quinlan_lab_rotation/all_chr_reference/\n",
    "wget https://storage.googleapis.com/gnomad-public/release/2.1.1/vcf/exomes/gnomad.exomes.r2.1.1.sites.vcf.bgz\n",
    "gunzip -c gnomad.exomes.r2.1.1.sites.vcf.bgz > gnomad.exomes.r2.1.1.sites.vcf\n",
    "    \n",
    "WGS Gnomad file: \n",
    "wget https://storage.googleapis.com/gnomad-public/release/2.1.1/vcf/genomes/gnomad.genomes.r2.1.1.exome_calling_intervals.sites.vcf.bgz\n",
    "gunzip -c gnomad.genomes.r2.1.1.exome_calling_intervals.sites.vcf.bgz > gnomad.genomes.r2.1.1.exome_calling_intervals.sites.vcf\n",
    "\n",
    "GFF/GTF file: \n",
    "wget ftp://ftp.ensembl.org/pub/grch37/current/gff3/homo_sapiens/Homo_sapiens.GRCh37.87.gff3.gz\n",
    "\n",
    "Exome Sequencing: \n",
    "cd ~/git/quinlan_lab_rotation/all_chr_reference/coverage/exome\n",
    "wget -c https://storage.cloud.google.com/gnomad-public/release/2.1/coverage/exomes/gnomad.exomes.coverage.summary.tsv.bgz\n",
    "gunzip -c gnomad.exomes.coverage.summary.tsv.bgz > gnomad.exomes.coverage.summary.tsv\n",
    "\n",
    "Whole Genome Sequencing: \n",
    "cd ~/git/quinlan_lab_rotation/all_chr_reference/coverage/wgs/\n",
    "wget https://storage.cloud.google.com/gnomad-public/release/2.1/coverage/genomes/gnomad.genomes.coverage.summary.tsv.bgz\n",
    "gunzip -c gnomad.genomes.coverage.summary.tsv.bgz > gnomad.genomes.coverage.summary.tsv\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: new_CCR_sift_cutoff_0.05_polyphen_cutoff_0.908_no_vaf_filter.bed\n"
     ]
    }
   ],
   "source": [
    "import cyvcf2\n",
    "import sys\n",
    "import exter ## exter.py file has to be in the working directory \n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.stats import percentileofscore\n",
    "import subprocess\n",
    "import os\n",
    "import fnmatch\n",
    "import csv\n",
    "from IPython.display import SVG\n",
    "from sklearn import preprocessing\n",
    "\n",
    "## Define the gff and vcf file path \n",
    "gff_path = \"/Users/jasonkunisaki/git/quinlan_lab_rotation/all_chr_reference/Homo_sapiens.GRCh37.82.chr20.gff3\" \n",
    "exome_vcf_path = \"/Users/jasonkunisaki/git/quinlan_lab_rotation/all_chr_reference/gnomad.exomes.r2.1.1.sites.vcf\"\n",
    "wgs_vcf_path = \"/Users/jasonkunisaki/git/quinlan_lab_rotation/all_chr_reference/gnomad.genomes.r2.1.1.exome_calling_intervals.sites.vcf\"\n",
    "exome_coverage_file = \"~/git/quinlan_lab_rotation/all_chr_reference/coverage/exome/gnomad.exomes.coverage.summary.tsv\"\n",
    "wgs_coverage_file = \"~/git/quinlan_lab_rotation/all_chr_reference/coverage/wgs/gnomad.genomes.coverage.summary.tsv \"\n",
    "\n",
    "## Define SIFT/PolyPhen filter parameters \n",
    "sift_polyphen_filter = False\n",
    "sift_cutoff = 0.05\n",
    "polyphen_cutoff = 0.908\n",
    "\n",
    "## Define VAF filter parameters \n",
    "vaf_filter = False\n",
    "vaf_cutoff = 50 ## Percentile to use \n",
    "\n",
    "## Assign the pathoscore key name\n",
    "if sift_polyphen_filter: \n",
    "    pathoscore_name = \"all_chr_CCR_\" + \"sift_cutoff_\" + str(sift_cutoff) + \"_polyphen_cutoff_\" + str(polyphen_cutoff)\n",
    "if not sift_polyphen_filter: \n",
    "    pathoscore_name = \"all_chr_CCR_no_sift_polyphen_filter\"\n",
    "if vaf_filter: \n",
    "    pathoscore_name = pathoscore_name + \"_vaf_cutoff_\" + str(vaf_cutoff) + \"th_percentile\"\n",
    "if not vaf_filter: \n",
    "    pathoscore_name = pathoscore_name + \"_no_vaf_filter\"\n",
    "print(\"File name: \" + pathoscore_name + \".bed\")\n",
    "\n",
    "## Assign test gene name \n",
    "test_key = \"KCNQ2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that defines functional variants; see [manuscript](https://www.nature.com/articles/s41588-018-0294-6.epdf?author_access_token=N8RkVYcavtplcSSE2KJkNdRgN0jAjWel9jnR3ZoTv0OG-o3UTB17cpoBs8B6XMBCl-5E0ZvpOii0iPl_hRSMGjWfkG4em7gjy95eV4bkTb0AF-E_dj3obeJfaadTja3aj9hUh1xk_BIztWgJScFx8w%3D%3D) for more details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to identify functional mutations\n",
    "def isfunctional(csq):\n",
    "    return any(c in csq['Consequence'] for c in ('stop_gained', 'stop_lost', 'start_lost', \n",
    "                                                 'initiator_codon_variant', 'rare_amino_acid_variant', \n",
    "                                                 'missense_variant', 'protein_altering_variant', \n",
    "                                                 'frameshift_variant', 'inframe_insertion', 'inframe_deletion')) \\\n",
    "    or (('splice_donor_variant' in csq['Consequence'] or 'splice_acceptor_variant' in csq['Consequence']) \\\n",
    "        and 'coding_sequence_variant' in csq['Consequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to read through a VCF file and obtain all variants. This step requires the [cyvcf2](https://github.com/brentp/cyvcf2/tree/master/cyvcf2) function to read and process the vcf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processVCF(vcf_path, gff_path, sift_polyphen_filter, sift_cutoff, polyphen_cutoff, type=\"exome\"):\n",
    "    ## Read in the VCF file \n",
    "    vcf = cyvcf2.VCF(vcf_path)\n",
    "    \n",
    "    ## Get the VEP description fields into a 'list'\n",
    "    kcsq = vcf[\"vep\"][\"Description\"].split(\":\")[1].strip(' \"').split(\"|\")\n",
    "    \n",
    "    ## Read in the gff transcript file and subset calls from chr20\n",
    "    transcripts = exter.read_gff(gff_path)\n",
    "\n",
    "    ## Make an empty dictionary \n",
    "    local_by_gene = defaultdict(list)\n",
    "    \n",
    "    ## For each variant in the VCF file: \n",
    "    for variant in vcf:\n",
    "        if variant.CHROM != \"X\" or variant.CHROM != \"Y\": continue ## Remove sex chromosomes (for now)\n",
    "        \n",
    "        ## Remove variants that have \"None, PASS, SEGDUP, or LCR\" as the filter result\n",
    "        if not (variant.FILTER is None or variant.FILTER in [\"PASS\", \"SEGDUP\", \"LCR\"]): continue\n",
    "        \n",
    "        ## Get the variant 'INFO' field from the VCF file for each variant\n",
    "        info = variant.INFO\n",
    "        \n",
    "        ## Set the functional consequence of the variant to 'False'\n",
    "        any_functional = False\n",
    "        \n",
    "        ## Merge the kcsq description keys with \n",
    "        csqs = [dict(zip(kcsq, c.split(\"|\"))) for c in info['vep'].split(\",\")]\n",
    "\n",
    "        ## For all variant/consequences that are within protein_coding regions\n",
    "        for csq in (c for c in csqs if c['BIOTYPE'] == 'protein_coding'):\n",
    "            ## Ignore intronic variants \n",
    "            if csq['Feature'] == '' or csq['EXON'] == '': continue #or csq['cDNA_position'] == '': continue\n",
    "            \n",
    "            ## Check to see if the variant's consequence is functional (based on the mutation types listed above)\n",
    "            if not isfunctional(csq): continue\n",
    "            \n",
    "            ## Assigns True to the variant as functional if it is NOT intronic and is deemed as functional by the 'isfunctional' function\n",
    "            any_functional = True\n",
    "\n",
    "            ## Get the exonic positions of the variants\n",
    "            local = transcripts.localize(variant.start, variant.end)\n",
    "\n",
    "            ## Get the VAF of the variant\n",
    "            vaf = variant.INFO.get('AF')\n",
    "\n",
    "            ## Get the SIFT/PolyPhen results (IS THIS RIGHT)\n",
    "            SIFT = csq[\"SIFT\"]\n",
    "            PolyPhen = csq[\"PolyPhen\"]\n",
    "                \n",
    "            ## Get the reference allele \n",
    "            ref_allele = variant.REF\n",
    "\n",
    "            ## Get string for the mutation \n",
    "            mut = []\n",
    "            for alt in variant.ALT: \n",
    "                mut.append(ref_allele + \">\" + alt) \n",
    "\n",
    "            ## Perform SIFT and Polyphen filters to remove the least delterious/damaging variant(s)\n",
    "            ## Go through each pair of scores and check if the varaint has SIFT and PolyPhen scores         \n",
    "            if sift_polyphen_filter == True: \n",
    "                sift_score = sift_cutoff + 0.1\n",
    "                polyphen_score = polyphen_cutoff - 0.1\n",
    "                if re.findall(\"deleterious\", SIFT):\n",
    "                    sift_score = float(re.split('\\)', re.split('\\(', SIFT)[1])[0])\n",
    "                if re.findall(\"damaging\", PolyPhen):\n",
    "                    polyphen_score = float(re.split('\\)', re.split('\\(', PolyPhen)[1])[0])\n",
    "                ## SIFT score has to be below the sift_cutoff to pass \n",
    "                ## PolyPhen score has to be above the polyphen_cutoff to pass \n",
    "                if not sift_score < sift_cutoff and polyphen_score > polyphen_cutoff:\n",
    "                    continue\n",
    "            if sift_polyphen_filter == False: \n",
    "                sift_score = SIFT\n",
    "                polyphen_score = PolyPhen\n",
    "            \n",
    "            for l in local:\n",
    "                l[\"chr\"] = variant.CHROM ## Get the chromosome \n",
    "                l[\"chr_start\"] = variant.start\n",
    "                l[\"chr_stop\"] = variant.end\n",
    "                l[\"mutation\"] = mut\n",
    "                l[\"vaf\"] = vaf\n",
    "                l[\"SIFT\"] = sift_score\n",
    "                l[\"PolyPhen\"] = polyphen_score\n",
    "                l[\"type\"] = type\n",
    "                local_by_gene[l[\"gene\"]].append(l)\n",
    "            break\n",
    "    return(local_by_gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "b'vep'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-314-368489ff91e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Read through gnomad WGS vcf variant file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwgs_local_by_gene\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessVCF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvcf_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwgs_vcf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgff_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgff_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msift_polyphen_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msift_polyphen_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msift_cutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msift_cutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolyphen_cutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolyphen_cutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"whole genome sequencing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-313-c6c607263fc9>\u001b[0m in \u001b[0;36mprocessVCF\u001b[0;34m(vcf_path, gff_path, sift_polyphen_filter, sift_cutoff, polyphen_cutoff, type)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m## Get the VEP description fields into a 'list'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mkcsq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvcf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vep\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' \"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m## Read in the gff transcript file and subset calls from chr20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/cyvcf2/cyvcf2.pyx\u001b[0m in \u001b[0;36mcyvcf2.cyvcf2.VCF.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/cyvcf2/cyvcf2.pyx\u001b[0m in \u001b[0;36mcyvcf2.cyvcf2.VCF.get_header_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: b'vep'"
     ]
    }
   ],
   "source": [
    "## Read through gnomad WGS vcf variant file \n",
    "wgs_local_by_gene = processVCF(vcf_path=wgs_vcf_path, gff_path=gff_path, sift_polyphen_filter=sift_polyphen_filter, sift_cutoff=sift_cutoff, polyphen_cutoff=polyphen_cutoff, type=\"whole genome sequencing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "b'vep'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-315-6244cfb0c4c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Read through gnomad EXOME vcf variant file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexome_local_by_gene\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessVCF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvcf_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexome_vcf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgff_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgff_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msift_polyphen_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msift_polyphen_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msift_cutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msift_cutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolyphen_cutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolyphen_cutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"exome sequencing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-313-c6c607263fc9>\u001b[0m in \u001b[0;36mprocessVCF\u001b[0;34m(vcf_path, gff_path, sift_polyphen_filter, sift_cutoff, polyphen_cutoff, type)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m## Get the VEP description fields into a 'list'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mkcsq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvcf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vep\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' \"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m## Read in the gff transcript file and subset calls from chr20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/cyvcf2/cyvcf2.pyx\u001b[0m in \u001b[0;36mcyvcf2.cyvcf2.VCF.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/cyvcf2/cyvcf2.pyx\u001b[0m in \u001b[0;36mcyvcf2.cyvcf2.VCF.get_header_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: b'vep'"
     ]
    }
   ],
   "source": [
    "## Read through gnomad EXOME vcf variant file\n",
    "exome_local_by_gene = processVCF(vcf_path=exome_vcf_path, gff_path=gff_path, sift_polyphen_filter=sift_polyphen_filter, sift_cutoff=sift_cutoff, polyphen_cutoff=polyphen_cutoff, type=\"exome sequencing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter variants based on coverage cutoffs \n",
    "See [CCR github repo](https://github.com/quinlan-lab/ccr/blob/master/getfiles.sh) for information on the original files used. To pass the coverage filter, greater than **50% of the samples have to have > 10X coverage at the variant's position.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to apply coverage filters \n",
    "def coverage_filter(data, coverage_file): \n",
    "    ## Read in the coverage file \n",
    "    coverage_df = pd.read_csv(coverage_file, sep=\"\\t\")\n",
    "    \n",
    "    ## Initialize the final dataset\n",
    "    filter_variants = defaultdict(list)\n",
    "    \n",
    "    for key in data.keys(): \n",
    "        subset_variants = data[key]\n",
    "        coverage_variants_per_gene = []\n",
    "    \n",
    "        ## Go through each variant within each gene and filter based on coverage\n",
    "        for variant in subset_variants: \n",
    "            chr_start = variant[\"chr_start\"]\n",
    "            chr_end = variant[\"chr_stop\"]\n",
    "                \n",
    "            ## Remove sites that do not meet the coverage cutoff \n",
    "            variant_coverage = coverage_df[(coverage_df.pos >= chr_start) & \n",
    "                                          (coverage_df.pos <= chr_end)]\n",
    "            coverage_10X = variant_coverage['over_10'].tolist()\n",
    "            if not all(i >= .50 for i in coverage_10X): continue\n",
    "            \n",
    "            ## Append variant to the coverage_variants_per_gene dataset\n",
    "            coverage_variants_per_gene.append(variant)\n",
    "        \n",
    "        ## Add variants to final dataset \n",
    "        filter_variants[key] = coverage_variants_per_gene\n",
    "        \n",
    "        prev_chr = chr\n",
    "    return(filter_variants)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_filter_df_exome = coverage_filter(data=exome_local_by_gene, coverage_file=exome_coverage_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_filter_df_wgs = coverage_filter(data=wgs_local_by_gene, coverage_file=wgs_coverage_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the VAF percentiles to determine appropriate cutoffs\n",
    "* Use all variants that are above the n-th percentile of VAFs \n",
    "* Get the VAF percentiles from exome and whole genome variants separately "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Function to obtain list of vaf values from exome and wgs variants \n",
    "def get_vaf_values(vcf): \n",
    "    vaf_list = []\n",
    "    for key in vcf.keys(): \n",
    "        for variant in vcf[key]: \n",
    "            vaf_list.append(variant[\"vaf\"])\n",
    "    return(vaf_list)\n",
    "\n",
    "## Function to apply vaf filter on exome and wgs variants \n",
    "def apply_vaf_filter(vcf, cutoff):  \n",
    "    vaf_all_genes = defaultdict(list)\n",
    "    for key in vcf.keys():\n",
    "        vaf_by_gene = []\n",
    "        for variant in vcf[key]: \n",
    "            if variant[\"vaf\"] < cutoff: \n",
    "                continue\n",
    "            vaf_by_gene.append(variant)\n",
    "        vaf_all_genes[key] = vaf_by_gene\n",
    "    return(vaf_all_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not vaf_filter: \n",
    "    exome_variants = cov_filter_df_exome\n",
    "    wgs_variants = cov_filter_df_wgs\n",
    "if vaf_filter: \n",
    "    ## Get the VAF values from exome and wgs samples\n",
    "    exome_vaf = get_vaf_values(vcf=cov_filter_df_exome)\n",
    "    exome_vaf = np.array(exome_vaf)\n",
    "    wgs_vaf = get_vaf_values(vcf=cov_filter_df_wgs)   \n",
    "    wgs_vaf = np.array(wgs_vaf)\n",
    "    \n",
    "    ## Get the numpy percentiles of vaf values \n",
    "    exome_vaf_cutoff = np.percentile(exome_vaf, vaf_cutoff)\n",
    "    wgs_vaf_cutoff = np.percentile(wgs_vaf, vaf_cutoff)\n",
    "    \n",
    "    ## Apply vaf filter on exome data \n",
    "    exome_variants = apply_vaf_filter(vcf=cov_filter_df_exome, cutoff=exome_vaf_cutoff)\n",
    "    \n",
    "    ## Apply vaf filter on wgs data \n",
    "    wgs_variants = apply_vaf_filter(vcf=cov_filter_df_wgs, cutoff=wgs_vaf_cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter variants based on segmental duplication and self-chains \n",
    "See [CCR github repo](https://github.com/quinlan-lab/ccr/blob/master/getfiles.sh) for information on the original files used. Creating the bed files requires the process (or a variation of the process) described below: \n",
    "\n",
    "**!!!!!!!!!! TO DO: Get bedtools to work so that the bedfiles can be merged !!!!!!!!!!** \n",
    "\n",
    "```\n",
    "Segmental Duplications: \n",
    "cd ~/git/quinlan_lab_rotation/reference/segmental_duplications\n",
    "wget ftp://hgdownload.soe.ucsc.edu/goldenPath/hg19/database/genomicSuperDups.txt.gz\n",
    "zcat < genomicSuperDups.txt.gz | cut -f 2-4 | sed 's/^chr//g' | sort -k1,1 -k2,2n > segmental_duplication_file.bed\n",
    "## zcat < genomicSuperDups.txt.gz | cut -f 2-4 | sed 's/^chr//g' | sort -k1,1 -k2,2n | bedtools merge | bgzip -c > data/segmental.bed.gz; cd data; tabix -f segmental.bed.gz\n",
    "\n",
    "Self Chains: \n",
    "cd ~/git/quinlan_lab_rotation/reference/self_chains\n",
    "wget ftp://hgdownload.cse.ucsc.edu/goldenpath/hg19/database/chainSelf.txt.gz\n",
    "gunzip chainSelf.txt.gz\n",
    "python get-chain.py | sed 's/^chr//g' | sort -k1,1 -k2,2n > selfchains_final.bed\n",
    "## python get-chain.py | sed 's/^chr//g' | sort -k1,1 -k2,2n | bedtools merge | bgzip -c > data/self-chains.id90.bed.gz; cd data; tabix -f self-chains.id90.bed.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the segmental duplication and self-chain data \n",
    "segdup_data = pd.read_csv(\"/Users/jasonkunisaki/git/quinlan_lab_rotation/reference/segmental_duplications/segmental_duplication_file.bed\", sep='\\t', header=None) \n",
    "selfchain_data = pd.read_csv(\"/Users/jasonkunisaki/git/quinlan_lab_rotation/reference/self_chains/selfchains_final.bed\", sep='\\t')\n",
    "remove_variants = selfchain_data\n",
    "remove_variants.append(segdup_data)\n",
    "remove_variants.columns = [\"chr\", \"start\", \"end\"]\n",
    "\n",
    "## Function to apply segmental duplication and self-chain filters\n",
    "def remove_segdup_selfchains(data, remove_variants=remove_variants):\n",
    "    filter_variants = defaultdict(list)\n",
    "    for key in data.keys(): \n",
    "        subset_variants = data[key] ## Subset variants by gene \n",
    "        variants_non_segdup_selfchain_per_gene = []\n",
    "        for variant in subset_variants: \n",
    "            chromosome = variant[\"chr\"]\n",
    "            chr_start = variant[\"chr_start\"]\n",
    "            chr_end = variant[\"chr_stop\"]\n",
    "\n",
    "            ## Subset remove_variants df based on chr\n",
    "            temp_rm_variants = remove_variants[remove_variants.chr==chromosome]\n",
    "            temp_rm_variants = temp_rm_variants[(((temp_rm_variants.start <= chr_start) & \n",
    "                                                (temp_rm_variants.end >= chr_start)) | ## if the variant's start is within the bad region \n",
    "                                               ((temp_rm_variants.start <= chr_end) & \n",
    "                                                (temp_rm_variants.end >= chr_end)) | ## if the variant's end is within the bad region\n",
    "                                                ((temp_rm_variants.start >= chr_start) &\n",
    "                                                temp_rm_variants.end <= chr_end))] ## if the bad region is within the variant's boundaries\n",
    "\n",
    "            ## Remove variants if the chr_start/end positions are within the remove_variants windows\n",
    "            if len(temp_rm_variants) > 0: \n",
    "                #print(variant)\n",
    "                continue \n",
    "            if len(temp_rm_variants) == 0: \n",
    "                variants_non_segdup_selfchain_per_gene.append(variant)\n",
    "        \n",
    "        filter_variants[key] = variants_non_segdup_selfchain_per_gene\n",
    "    return(filter_variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_segdup_selfchain_exome = remove_segdup_selfchains(data=exome_variants, remove_variants=remove_variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_segdup_selfchain_wgs = remove_segdup_selfchains(data=wgs_variants, remove_variants=remove_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the exome and whole genome variants that passed the above filters: \n",
    "* VAF cutoff \n",
    "* SIFT/PolyPhen scores \n",
    "* Remove segmental duplications and self-chains \n",
    "* Coverage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine wgs and exome variant data. Duplicate variants will be accounted for below. \n",
    "combined_wgs_exome_variants = defaultdict(list)\n",
    "for key in no_segdup_selfchain_wgs.keys(): \n",
    "    if len(no_segdup_selfchain_exome[key]) == 0: \n",
    "        combined_wgs_exome_variants[key] = no_segdup_selfchain_wgs[key]\n",
    "    else: \n",
    "        combined_wgs_exome_variants[key] = no_segdup_selfchain_exome[key] + no_segdup_selfchain_wgs[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_wgs_exome_variants[test_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform quality check on pathogenic variants. Remove genes with no variants after applying filters from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_variant_vcf = defaultdict(list)\n",
    "## Check if there is at least one variant in each gene \n",
    "for key in combined_wgs_exome_variants.keys():\n",
    "    if len(combined_wgs_exome_variants[key]) > 0:\n",
    "        final_variant_vcf[key] = combined_wgs_exome_variants[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix overlapping variants within each gene + apply SIFT/PolyPhen and VAF filters\n",
    "| Position |   V1   |   V2   |   V3   |   V4   |   V5   |   V6   |   V7   |   V8   |   V9   |  V10   |  V11   |\n",
    "| :------- | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n",
    "| Start (0-based)    | 126244 | 126244 | 126253 | 126255 | 126258 | 126258 | 126260 | 126264 | 126264 | 126265 | **126279** |\n",
    "| End (1-based)      | 126245 | 126254 | 126254 | 126256 | 126259 | 126262 | **126279** | 126265 | 126265 | 126266 | 126280 |\n",
    "\n",
    "### Combine variant positions for:\n",
    "+ adjacent bases: **V7 ends at 126279 (1-based) and V11 starts at 126279 (0-based). The variant windows are combined.**\n",
    "+ equivalent variants (in terms of position) \n",
    "+ overlapping variants\n",
    "\n",
    "### The determined variant range should encompass all of the above variants\n",
    "| Position | window 1 | window 2 | window 3 | \n",
    "| :------: | :------: | :------: | :------: |\n",
    "| Start    | 126244   | 126255   | 126258   |\n",
    "| End      | 126254   | 126256   | 126280   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inititalize the master list \n",
    "unique_variant_pos_all_genes = defaultdict(list)\n",
    "for key in final_variant_vcf.keys():\n",
    "    subset_variants = final_variant_vcf[key]\n",
    "    ## Get all of the start and stop variant positions within the gene\n",
    "    start_positions = [d[\"chr_start\"] for d in subset_variants]\n",
    "    end_positions = [d[\"chr_stop\"] for d in subset_variants]\n",
    "    exonic_end_positions = [d[\"stop\"] for d in subset_variants]\n",
    "\n",
    "    ## Go throouh each start/end position and check if there are overlapping variants \n",
    "    unique_variant_pos_per_gene = []\n",
    "    positions = []\n",
    "    for start, end in zip(start_positions, end_positions):\n",
    "        ## Get variants in which the start is within the variant's start/end\n",
    "        temp1 = list(filter(lambda subset_variants: \n",
    "               subset_variants[\"chr_start\"] <= start and subset_variants[\"chr_stop\"] >= start, subset_variants))\n",
    "        \n",
    "        ## Check to see if there are variants that overlap with maximum variant's end position from the above subset\n",
    "        new_end = max([d[\"chr_stop\"] for d in temp1])\n",
    "        temp2 = list(filter(lambda subset_variants:\n",
    "                   subset_variants[\"chr_start\"] <= new_end and subset_variants[\"chr_stop\"] >= new_end, subset_variants))\n",
    "        \n",
    "        ## Combine all variants that overlap with the intial variant(s) in question\n",
    "        for v in temp2: \n",
    "            if v not in temp1: \n",
    "                temp1.append(v)\n",
    "\n",
    "        ## Get the index at which an adjacent variant's start position overlaps with the end position of the adjusted window\n",
    "        ## Assign the maximum end pos of the adjacent variant as the window's stop position\n",
    "        while max(set([d[\"chr_stop\"] for d in temp1])) in start_positions and \\\n",
    "        max(set([d[\"chr_stop\"] for d in temp1])) not in positions: ## Make sure it does not pull a variant that was already accounted for in a window\n",
    "            d = defaultdict(list)\n",
    "            for index, e in enumerate(start_positions):\n",
    "                d[e].append(index)\n",
    "            \n",
    "            ## Get all the end positions of adjacent variants \n",
    "            end_index = d[max(set([d[\"chr_stop\"] for d in temp1]))]\n",
    "            \n",
    "            ## Replace the maximum of the adjusted window with the adjacent variant's stop\n",
    "            temp1[len(temp1)-1][\"stop\"] = max(list(np.array(exonic_end_positions)[end_index]))\n",
    "            temp1[len(temp1)-1][\"chr_stop\"] = max(list(np.array(end_positions)[end_index]))\n",
    "\n",
    "        ## If the variant is NOT the first variant \n",
    "        if len(positions) > 0: \n",
    "            ## Check to see if the variant's start and stop falls in a window already calculated \n",
    "            prev_start_positions = [d[\"final_start_positions\"] for d in positions] ## Get start positions\n",
    "            prev_end_positions = [d[\"final_end_positions\"] for d in positions] ## Get end positions\n",
    "            for s, e in zip(prev_start_positions, prev_end_positions):\n",
    "                ## If the variant' start or end falls within a previously calculated window, skip the variant \n",
    "                if (start >= s and start <= e) or (end >= s and start <= e):\n",
    "                    temp1 = []\n",
    "                    break\n",
    "\n",
    "        ## If there are overlapping variants\n",
    "        if len(temp1) > 1: \n",
    "            final_temp = {\n",
    "                \"gene\": key,\n",
    "                \"start\": min(set([d[\"start\"] for d in temp1])),\n",
    "                \"stop\": max(set([d[\"stop\"] for d in temp1])),\n",
    "                \"strand\": temp1[1][\"strand\"],\n",
    "                \"exon\": temp1[1][\"exon\"],\n",
    "                \"chr\": temp1[1][\"chr\"],\n",
    "                \"chr_start\": min(set([d[\"chr_start\"] for d in temp1])),\n",
    "                \"chr_stop\": max(set([d[\"chr_stop\"] for d in temp1])),\n",
    "                \"mutation\": \"overlapping mutations\",\n",
    "                \"vaf\": temp1[1][\"vaf\"],\n",
    "                \"SIFT\": temp1[1][\"SIFT\"],\n",
    "                \"PolyPhen\": temp1[1][\"PolyPhen\"]\n",
    "            }\n",
    "            unique_variant_pos_per_gene.append(final_temp)\n",
    "\n",
    "        ## If there are no overlapping variants \n",
    "        if len(temp1) == 1:\n",
    "            final_temp = temp1[0]\n",
    "            unique_variant_pos_per_gene.append(final_temp)\n",
    "\n",
    "        ## Get the complete start and stop positions for variants within gene\n",
    "        test = {\n",
    "        \"final_start_positions\": final_temp[\"chr_start\"], ## 0-based\n",
    "        \"final_end_positions\": final_temp[\"chr_stop\"] ## 1-based \n",
    "        } \n",
    "\n",
    "        ## Store the start and stop positions for each adjusted variant range\n",
    "        if len(temp1) > 0:\n",
    "            positions.append(test)\n",
    "    \n",
    "    ## Sort each variant within each gene based on the genomic start position\n",
    "    unique_variant_pos_per_gene = sorted(unique_variant_pos_per_gene, key = lambda i: i[\"chr_start\"])\n",
    "    unique_variant_pos_all_genes[key] = unique_variant_pos_per_gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_variant_pos_all_genes[test_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caclulate CCR window sizes and positions based on variant coordinates\n",
    "```\n",
    "   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19  20    ## 1 base\n",
    " | T | T | G | A | T | C | G | A | T | A | G | C | T | A | C | C | G | A | T | C |\n",
    " 0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19  20  ## 0 base \n",
    "```\n",
    "**Examples**\n",
    "+ **1 base b/w variants:** 2 *(1-based; variant_plus_one.start; 2-3 G)* - 1 *(0-based; variant.end; 0-1 T)* = 1\n",
    "+ **Multiple bases b/w variants**: 10 *(0-based; variant_plus_one.start; 10-11 G)* - 4 *(1-based; variant.end; 0-4 TTGA)* = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store the genes as keys \n",
    "## Store the window sizes/variant positions in dictionaries  \n",
    "final_window_dataset = {}\n",
    "for key in unique_variant_pos_all_genes.keys(): \n",
    "    ## Subset dictionaries (i.e. list of variants) based on each key (i.e. each gene)\n",
    "    subset_variants = unique_variant_pos_all_genes[key]\n",
    "    \n",
    "    ## Go through the variants within each gene and measure the CCR window size \n",
    "    subset_window_list = []\n",
    "    \n",
    "    ## Go through each variant, get the variant pair, and then calculate window size \n",
    "    for variant, variant_plus_one in zip(subset_variants, subset_variants[1:]):\n",
    "        ## Calculate window position of the variant\n",
    "        variant1_window = {\"gene\": key,\n",
    "        \"chromosome\": \"chr\" + variant[\"chr\"],\n",
    "        \"variant1_start\": variant[\"start\"],\n",
    "        \"variant1_stop\": variant[\"stop\"],\n",
    "        \"variant2_start\": \"NA\",\n",
    "        \"variant2_stop\": \"NA\",\n",
    "        \"variant1_vaf\": variant[\"vaf\"],\n",
    "        \"variant2_vaf\": \"NA\",\n",
    "        \"variant1_mut\": variant[\"mutation\"],\n",
    "        \"variant2_mut\": \"NA\",\n",
    "        \"variant1_SIFT\": variant[\"SIFT\"],\n",
    "        \"variant1_PolyPhen\": variant[\"PolyPhen\"],\n",
    "        \"variant2_SIFT\": \"NA\",\n",
    "        \"variant2_PolyPhen\": \"NA\",\n",
    "        \"window_size\": 0,\n",
    "        \"window_start\": variant[\"chr_start\"],\n",
    "        \"window_stop\": variant[\"chr_stop\"],\n",
    "        \"varflag\": \"VARTRUE\"\n",
    "        }\n",
    "\n",
    "        ## Calculate window positions between variants\n",
    "        window_dict = {\"gene\": key,\n",
    "        \"chromosome\": \"chr\" + variant[\"chr\"],\n",
    "        \"variant1_start\": variant[\"start\"],\n",
    "        \"variant1_stop\": variant[\"stop\"],\n",
    "        \"variant2_start\": variant_plus_one[\"start\"],\n",
    "        \"variant2_stop\": variant_plus_one[\"stop\"],\n",
    "        \"variant1_vaf\": variant[\"vaf\"],\n",
    "        \"variant2_vaf\": variant_plus_one[\"vaf\"],\n",
    "        \"variant1_mut\": variant[\"mutation\"],\n",
    "        \"variant2_mut\": variant_plus_one[\"mutation\"],\n",
    "        \"variant1_SIFT\": \"NA\",\n",
    "        \"variant1_PolyPhen\": \"NA\",\n",
    "        \"variant2_SIFT\": \"NA\",\n",
    "        \"variant2_PolyPhen\": \"NA\",\n",
    "        \"window_size\": max(0, variant_plus_one[\"start\"] - variant[\"stop\"]),\n",
    "        \"window_start\": variant[\"chr_stop\"],\n",
    "        \"window_stop\": variant_plus_one[\"chr_start\"], \n",
    "        \"varflag\": \"VARFALSE\"\n",
    "        }\n",
    "\n",
    "        ## Append window for variant and adjacent variants\n",
    "        subset_window_list.append(variant1_window)\n",
    "        if window_dict[\"window_size\"] > 0 :\n",
    "            subset_window_list.append(window_dict)\n",
    "\n",
    "    ## Get the information for the last variant in the GOI \n",
    "    last_variant = subset_variants[len(subset_variants)-1]\n",
    "    last_variant = {\"gene\": key,\n",
    "        \"chromosome\": \"chr\" + last_variant[\"chr\"],\n",
    "        \"variant1_start\": last_variant[\"start\"],\n",
    "        \"variant1_stop\": last_variant[\"stop\"],\n",
    "        \"variant2_start\": \"NA\",\n",
    "        \"variant2_stop\": \"NA\",\n",
    "        \"variant1_vaf\": last_variant[\"vaf\"],\n",
    "        \"variant2_vaf\": \"NA\",\n",
    "        \"variant1_mut\": last_variant[\"mutation\"],\n",
    "        \"variant2_mut\": \"NA\",\n",
    "        \"variant1_SIFT\": last_variant[\"SIFT\"],\n",
    "        \"variant1_PolyPhen\": last_variant[\"PolyPhen\"],\n",
    "        \"variant2_SIFT\": \"NA\",\n",
    "        \"variant2_PolyPhen\": \"NA\",\n",
    "        \"window_size\": 0,\n",
    "        \"window_start\": last_variant[\"chr_start\"],\n",
    "        \"window_stop\": last_variant[\"chr_stop\"],\n",
    "        \"varflag\": \"VARTRUE\"\n",
    "        }\n",
    "    \n",
    "    ## Append window for the last variant \n",
    "    subset_window_list.append(last_variant)\n",
    "\n",
    "    ## Append each gene's window data to the associated gene in the final dataset\n",
    "    assert(key not in final_window_dataset)\n",
    "    final_window_dataset[key] = subset_window_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_window_dataset[test_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain the percentiles for each CCR window \n",
    "## Calculate percentiles based on varflag == VARFALSE value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the 'windows' list  \n",
    "windows = []\n",
    "\n",
    "## Go through each gene and get the window sizes\n",
    "for key in final_window_dataset.keys():\n",
    "    subset_variants = final_window_dataset[key]\n",
    "    for variant in subset_variants:\n",
    "#         if variant[\"varflag\"] == \"VARFALSE\":\n",
    "#             windows.append(variant[\"window_size\"])\n",
    "          windows.append(variant[\"window_size\"])\n",
    "\n",
    "## Sort the 'windows' list\n",
    "windows.sort()\n",
    "\n",
    "## Convert windows to numpy array\n",
    "windows = np.array(windows)\n",
    "\n",
    "## Go through each of the variants and obtain the scores \n",
    "final_dataset = defaultdict(list)\n",
    "for key in final_window_dataset.keys():\n",
    "    ## Get the variants found within each gene\n",
    "    subset_variants = final_window_dataset[key]\n",
    "    \n",
    "    ## Define a percentiles list with each window's percentile + other information as the dictionary\n",
    "    percentiles_single_gene = []\n",
    "    for variant in subset_variants:\n",
    "        ## If the variant is a variant, assign percentile as 0\n",
    "        if variant[\"varflag\"] == \"VARTRUE\": \n",
    "            variant[\"percentile\"] = 0\n",
    "        \n",
    "        ## If the variant is a CCR window, assign percentile based on window values \n",
    "        if variant[\"varflag\"] == \"VARFALSE\":        \n",
    "            variant[\"percentile\"] = round(percentileofscore(windows, variant[\"window_size\"]), 2)\n",
    "        \n",
    "        ## Append the dictionary to the list of percentiles for each gene\n",
    "        percentiles_single_gene.append(variant)\n",
    "    \n",
    "    ## Appened all percentiles across all genes to the final dataset list \n",
    "    final_dataset[key] = percentiles_single_gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset[test_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the weighted percentiles \n",
    "Start at 100 (the most constrained region) and scale down proportionally for each subsequent region by the fraction of the length of protein-coding exome already covered by the preceding region. More information can be found [here](https://quinlan-lab.github.io/ccr/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use panda dataframes to make BED files for PathoScore\n",
    "new_CCR = pd.DataFrame()\n",
    "for key in final_dataset.keys():\n",
    "    gene_pandaDf = pd.DataFrame(final_dataset[key], columns=[\"chromosome\", \"window_start\", \n",
    "        \"window_stop\", \"gene\", \n",
    "        \"window_size\", \"varflag\", \"variant1_vaf\", \"variant1_SIFT\", \"variant1_PolyPhen\", \"percentile\"])\n",
    "    new_CCR = pd.concat([new_CCR, gene_pandaDf])\n",
    "\n",
    "## Change the column names of the combined panda dataframes and remove window_size column\n",
    "new_CCR.columns = [\"#chrom\", \"start\", \"end\", \"gene\", \"window_size\", \"varflag\",\n",
    "                   \"vaf\", \"SIFT\", \"PolyPhen\", \"residual_pct\"]\n",
    "\n",
    "## Sort (descending) the panda dataframe by residual percentile \n",
    "new_CCR = new_CCR.sort_values(by = [\"residual_pct\", \"#chrom\", \"start\"], ascending = False)\n",
    "\n",
    "## Write the output file and read it back in to remove duplicate index values in pandas dataset \n",
    "output_file = pathoscore_name + \".bed\" \n",
    "new_CCR.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "new_CCR = pd.read_csv(pathoscore_name + \".bed\", sep=\"\\t\")\n",
    "\n",
    "## Print the head of new_CCR dataset\n",
    "new_CCR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the total length covered by CCRs\n",
    "totlength = sum(new_CCR[\"window_size\"].tolist())\n",
    "\n",
    "## Go throuh each row and determine the weighted percentile \n",
    "pct = 100.0\n",
    "opct = new_CCR.loc[0, \"residual_pct\"]\n",
    "weighted = [] ## Initialize the weighted percentile list \n",
    "for r in new_CCR.itertuples():\n",
    "    ## Do not perform this calculation on variants\n",
    "    if r[6] != \"VARTRUE\":  \n",
    "        regionlength = int(r[5]) ## Get the CCR size \n",
    "        residual_pct = r[len(r)-1] ## Get the residual CCR percentile \n",
    "        \n",
    "        ## If the percentile is not equal to the previous CCR's percentile\n",
    "        if r[len(r)-1] != opct: \n",
    "            pct -= regionlength/totlength*100 ## Perform the adjustment calculation \n",
    "            opct = residual_pct ## Assign new CCR percentile\n",
    "                             \n",
    "        ## Add weighted percentile to the weighted list \n",
    "        weighted.append(pct)\n",
    "    else: ## Assign 0th percentile to variants \n",
    "        weighted.append(0)\n",
    "        \n",
    "## TODO: check to see if these percentiles are the final weighted percentiles \n",
    "X_train = np.array(weighted).reshape(len(weighted), 1)\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,100))\n",
    "final_pctile = min_max_scaler.fit_transform(X_train)\n",
    "final_pctile_df = pd.DataFrame(final_pctile)\n",
    "\n",
    "## Add the weighted CCR values to the panda datase \n",
    "weighted_CCR = pd.concat([new_CCR, final_pctile_df], axis=1)\n",
    "weighted_CCR.columns = [\"#chrom\", \"start\", \"end\", \"gene\", \"window_size\", \"varflag\",\n",
    "                   \"vaf\", \"SIFT\", \"PolyPhen\", \"residual_pct\", \"ccr_pct\"]\n",
    "\n",
    "weighted_CCR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a BED file to run in PathoScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort by position\n",
    "weighted_CCR = weighted_CCR.sort_values(by=[\"#chrom\", \"start\", \"end\"])\n",
    "\n",
    "## Write the output file and read it back in to remove duplicate index values in pandas dataset \n",
    "output_file = pathoscore_name + \".bed\" \n",
    "weighted_CCR.to_csv(output_file, sep=\"\\t\", index=False) ## Write the bed file \n",
    "weighted_CCR.to_csv((pathoscore_name + \".txt\"), sep=\"\\t\", index=False) ## Write the text file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the script needed to run the PathoScore function on all bed files present in the directory \n",
    "\n",
    "Step 1: create the scripts needed to run PathoScore \n",
    "\n",
    "Step 2: bgzip and tabix all of the bed files present in the directory (TODO)\n",
    "\n",
    "Step 3: annotate benign and pathogenic variants from all bed files \n",
    "\n",
    "Step 4: run the PathoScore evaluate function on annotated benign and pathogenic files \n",
    "\n",
    "## Write the PathoScore scripts based on the bed files present in the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_files = os.listdir(\".\")\n",
    "pattern = \"*.bed\"\n",
    "## Create a list of bed files there associated column numbers \n",
    "bedfile_colnum_list = []\n",
    "for file in list_of_files: \n",
    "    if fnmatch.fnmatch(file, pattern):\n",
    "        data = pd.read_csv(file, sep='\\t')\n",
    "        colnames = list(data.columns)\n",
    "        bedfile_colnum_list.append({\n",
    "            \"filename\": \"~/git/quinlan_lab_rotation/\" + file + \".gz\",\n",
    "            \"filename_no_suffix\": file.split(\".bed\")[0],\n",
    "            \"colnumber\": colnames.index(\"ccr_pct\") + 1\n",
    "        })\n",
    "\n",
    "## Use the bedfile names and column numbers to generate pathoscore scripts\n",
    "score_script = []\n",
    "evaluate_script = []\n",
    "for i in range(len(bedfile_colnum_list)): \n",
    "    filename = bedfile_colnum_list[i][\"filename\"]\n",
    "    if not re.findall(\"all_chr\", filename): continue ## ignore files that do not use all chromosomes \n",
    "    filename_no_suffix = bedfile_colnum_list[i][\"filename_no_suffix\"]\n",
    "    colnum = str(bedfile_colnum_list[i][\"colnumber\"])\n",
    "    score_script.append(\"--scores \" + filename + \":\" + filename_no_suffix + \":\" + colnum + \":max\")\n",
    "    evaluate_script.append(\"-s \" + filename_no_suffix)\n",
    "separator = \" \"\n",
    "benign_score_script = \"python pathoscore.py annotate --prefix benign \" + separator.join(score_script) + \" ~/git/quinlan_lab_rotation/pathoscore/truth-sets/GRCh37/clinvar/clinvar-benign.20170905.vcf.gz\"\n",
    "pathogenic_score_script = \"python pathoscore.py annotate --prefix pathogenic \" + separator.join(score_script) + \" ~/git/quinlan_lab_rotation/pathoscore/truth-sets/GRCh37/clinvar/clinvar-pathogenic.20170905.vcf.gz\"\n",
    "pathoscore_evaluate_script = \"python pathoscore.py evaluate \" + separator.join(evaluate_script) + \" pathogenic.vcf.gz benign.vcf.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(benign_score_script)\n",
    "print(pathogenic_score_script)\n",
    "print(pathoscore_evaluate_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = open(r\"pathoscore_scripts.sh\", \"w+\")\n",
    "file_object.write(\"%s\\n%s\\n%s\\n\" % (benign_score_script, pathogenic_score_script, pathoscore_evaluate_script))\n",
    "file_object.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bgzip all of the bed files in the directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "bash /Users/jasonkunisaki/git/quinlan_lab_rotation/scripts/bgzip.sh /Users/jasonkunisaki/git/quinlan_lab_rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the PathoScore funtions that were generated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "bash /Users/jasonkunisaki/git/quinlan_lab_rotation/pathoscore_scripts.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move PathoScore output files to the appropriate output directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "bash /Users/jasonkunisaki/git/quinlan_lab_rotation/scripts/move_files.sh /Users/jasonkunisaki/git/quinlan_lab_rotation/pathoscore_results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print ROC curve from the PathoScore results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG('/Users/jasonkunisaki/git/quinlan_lab_rotation/pathoscore_results/pathoscore.roc.svg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
