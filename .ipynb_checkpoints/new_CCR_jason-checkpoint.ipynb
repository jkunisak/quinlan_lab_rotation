{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jason Kunisaki - Quinlan Lab Rotation\n",
    "## Evaluate CCRs\n",
    "\n",
    "This notebook contains the script needed to recapitulate CCRs from the first iteration of CCRs [(see manuscript)](https://www.nature.com/articles/s41588-018-0294-6.epdf?author_access_token=N8RkVYcavtplcSSE2KJkNdRgN0jAjWel9jnR3ZoTv0OG-o3UTB17cpoBs8B6XMBCl-5E0ZvpOii0iPl_hRSMGjWfkG4em7gjy95eV4bkTb0AF-E_dj3obeJfaadTja3aj9hUh1xk_BIztWgJScFx8w%3D%3D) and develop new filter parameters to better define CCRs. These parameters include **VAF, SIFT/PolyPhen scores, and coverage**. Use [PathoScore](https://github.com/quinlan-lab/pathoscore) to determine the efficacy of CCRs as a classifier for clinvar pathogenic variants. SIFT and PolyPhen interpretations can be found in the [VEP annotation descriptions](https://uswest.ensembl.org/info/genome/variation/prediction/protein_function.html).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cyvcf2\n",
    "import sys\n",
    "import exter ## exter.py file has to be in the working directory \n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.stats import percentileofscore\n",
    "import subprocess\n",
    "import os\n",
    "import fnmatch\n",
    "import csv\n",
    "from IPython.display import SVG\n",
    "\n",
    "## Define the gff and vcf file path \n",
    "gff_path = \"/Users/jasonkunisaki/git/quinlan_lab_rotation/reference/Homo_sapiens.GRCh37.82.chr20.gff3\" \n",
    "exome_vcf_path = \"/Users/jasonkunisaki/git/quinlan_lab_rotation/reference/gnomad.exomes.r2.1.1.sites.20.vcf.bgz\"\n",
    "wgs_vcf_path = \"/Users/jasonkunisaki/git/quinlan_lab_rotation/reference/gnomad.genomes.r2.1.1.exome_calling_intervals.sites.vcf.bgz\"\n",
    "\n",
    "## Define SIFT/PolyPhen filter parameters \n",
    "sift_polyphen_filter = True\n",
    "sift_cutoff = 0.05\n",
    "polyphen_cutoff = 0.5\n",
    "\n",
    "## Assign the pathoscore key name\n",
    "if sift_polyphen_filter: \n",
    "    pathoscore_name = \"new_CCR_\" + \"sift_cutoff_\" + str(sift_cutoff) + \"_polyphen_cutoff_\" + str(polyphen_cutoff)\n",
    "else: \n",
    "    pathoscore_name = \"new_CCR_no_sift_polyphen_filter\"\n",
    "    \n",
    "#pathoscore_name = \"new_CCR\"\n",
    "    \n",
    "## Assign test gene name \n",
    "test_key = \"GNAS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that defines functional variants; see [manuscript](https://www.nature.com/articles/s41588-018-0294-6.epdf?author_access_token=N8RkVYcavtplcSSE2KJkNdRgN0jAjWel9jnR3ZoTv0OG-o3UTB17cpoBs8B6XMBCl-5E0ZvpOii0iPl_hRSMGjWfkG4em7gjy95eV4bkTb0AF-E_dj3obeJfaadTja3aj9hUh1xk_BIztWgJScFx8w%3D%3D) for more details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to identify functional mutations\n",
    "def isfunctional(csq):\n",
    "    return any(c in csq['Consequence'] for c in ('stop_gained', 'stop_lost', 'start_lost', \n",
    "                                                 'initiator_codon_variant', 'rare_amino_acid_variant', \n",
    "                                                 'missense_variant', 'protein_altering_variant', \n",
    "                                                 'frameshift_variant', 'inframe_insertion', 'inframe_deletion')) \\\n",
    "    or (('splice_donor_variant' in csq['Consequence'] or 'splice_acceptor_variant' in csq['Consequence']) \\\n",
    "        and 'coding_sequence_variant' in csq['Consequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to read through a VCF file and obtain all variants. This step requires the [cyvcf2](https://github.com/brentp/cyvcf2/tree/master/cyvcf2) function to read and process the vcf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processVCF(vcf_path, gff_path, sift_polyphen_filter, sift_cutoff, polyphen_cutoff, type=\"exome\"):\n",
    "    ## Read in the VCF file \n",
    "    vcf = cyvcf2.VCF(vcf_path)\n",
    "    \n",
    "    ## Get the VEP description fields into a 'list'\n",
    "    kcsq = vcf[\"vep\"][\"Description\"].split(\":\")[1].strip(' \"').split(\"|\")\n",
    "    \n",
    "    ## Read in the gff transcript file and subset calls from chr20\n",
    "    transcripts = exter.read_gff(gff_path)\n",
    "    transcripts20 = transcripts[\"20\"] # get chr20\n",
    "\n",
    "    ## Make an empty dictionary \n",
    "    local_by_gene = defaultdict(list)\n",
    "    \n",
    "    ## For each variant in the VCF file: \n",
    "    for variant in vcf:\n",
    "        if variant.CHROM != \"20\": continue\n",
    "        ## Remove variants that have \"None, PASS, SEGDUP, or LCR\" as the filter result\n",
    "        ## NOTE: 'continue' will remove variants without these FILTER results\n",
    "        if not (variant.FILTER is None or variant.FILTER in [\"PASS\", \"SEGDUP\", \"LCR\"]):\n",
    "                continue\n",
    "        ## Get the variant 'INFO' field from the VCF file for each variant\n",
    "        info = variant.INFO\n",
    "        ## Set the functional consequence of the variant to 'False'\n",
    "        any_functional = False\n",
    "        ## Merge the kcsq description keys with \n",
    "        csqs = [dict(zip(kcsq, c.split(\"|\"))) for c in info['vep'].split(\",\")]\n",
    "\n",
    "        ## For all variant/consequences that are within protein_coding regions\n",
    "        for csq in (c for c in csqs if c['BIOTYPE'] == 'protein_coding'):\n",
    "            ## NOTE: 'continue' will ignore variants that are intronic \n",
    "            if csq['Feature'] == '' or csq['EXON'] == '': \n",
    "                continue #or csq['cDNA_position'] == '': continue\n",
    "            ## Check to see if the variant's consequence is functional (based on the mutation types listed above)\n",
    "            ## NOTE: 'continue' will ignore variants that are not functional\n",
    "            if not isfunctional(csq): \n",
    "                continue\n",
    "            ## Assigns True to the variant as functional if it is NOT intronic and is deemed as functional by the 'isfunctional' function\n",
    "            any_functional = True\n",
    "\n",
    "            ## Get the exonic positions of the variants\n",
    "            local = transcripts20.localize(variant.start, variant.end)\n",
    "\n",
    "            ## Get the VAF of the variant\n",
    "            vaf = variant.INFO.get('AF')\n",
    "\n",
    "            ## Get the SIFT/PolyPhen results (IS THIS RIGHT)\n",
    "            SIFT = csq[\"SIFT\"]\n",
    "            PolyPhen = csq[\"PolyPhen\"]\n",
    "\n",
    "            ## Get the reference allele \n",
    "            ref_allele = variant.REF\n",
    "\n",
    "            ## Get string for the mutation \n",
    "            mut = []\n",
    "            for alt in variant.ALT: \n",
    "                mut.append(ref_allele + \">\" + alt) \n",
    "\n",
    "            ## Remove variants that do not meet a specific VAF cutoff \n",
    "            # if vaf < 0.0000001: continue\n",
    "\n",
    "            ## Perform SIFT and Polyphen filters to remove the least delterious/damaging variant(s)\n",
    "            ## Go through each pair of scores and check if the varaint has SIFT and PolyPhen scores\n",
    "            sift_score = sift_cutoff + 0.1\n",
    "            polyphen_score = polyphen_cutoff - 0.1\n",
    "            if sift_polyphen_filter == True: \n",
    "                if re.findall(\"deleterious\", SIFT):\n",
    "                    sift_score = float(re.split('\\)', re.split('\\(', SIFT)[1])[0])\n",
    "                if re.findall(\"damaging\", PolyPhen):\n",
    "                    polyphen_score = float(re.split('\\)', re.split('\\(', PolyPhen)[1])[0])\n",
    "                ## SIFT score has to be below the sift_cutoff to pass \n",
    "                ## PolyPhen score has to be above the polyphen_cutoff to pass \n",
    "                if not sift_score < sift_cutoff and polyphen_score > polyphen_cutoff:\n",
    "                    continue\n",
    "\n",
    "            ## Remove variants that are found in duplicated segments \n",
    "            #if variant.INFO.get('segdup') == True or variant.INFO.get('lcr'): continue\n",
    "            \n",
    "            for l in local:\n",
    "                l[\"chr\"] = variant.CHROM ## Get the chromosome \n",
    "                l[\"chr_start\"] = variant.start\n",
    "                l[\"chr_stop\"] = variant.end\n",
    "                l[\"mutation\"] = mut\n",
    "                l[\"vaf\"] = vaf\n",
    "                l[\"SIFT\"] = sift_score\n",
    "                l[\"PolyPhen\"] = polyphen_score\n",
    "                l[\"type\"] = type\n",
    "                local_by_gene[l[\"gene\"]].append(l)\n",
    "            break\n",
    "    return(local_by_gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read through gnomad WGS vcf variant file \n",
    "wgs_local_by_gene = processVCF(vcf_path=wgs_vcf_path, gff_path=gff_path, sift_polyphen_filter=sift_polyphen_filter, sift_cutoff=sift_cutoff, polyphen_cutoff=polyphen_cutoff, type=\"whole genome sequencing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read through gnomad EXOME vcf variant file\n",
    "exome_local_by_gene = processVCF(vcf_path=exome_vcf_path, gff_path=gff_path, sift_polyphen_filter=sift_polyphen_filter, sift_cutoff=sift_cutoff, polyphen_cutoff=polyphen_cutoff, type=\"exome sequencing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine wgs and exome variant data. Duplicate variants will be accounted for below. \n",
    "combined_wgs_exome_variants = defaultdict(list)\n",
    "for key in wgs_local_by_gene.keys(): \n",
    "    if len(temp[key]) == 0: \n",
    "        combined_wgs_exome_variants[key] = wgs_local_by_gene[key]\n",
    "    else: \n",
    "        combined_wgs_exome_variants[key] = exome_local_by_gene[key] + wgs_local_by_gene[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_wgs_exome_variants[test_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform quality check on pathogenic variants. Remove genes with no variants after applying filters from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_variant_vcf = defaultdict(list)\n",
    "## Check if there is at least one variant in each gene \n",
    "for key in combined_wgs_exome_variants.keys():\n",
    "    if len(combined_wgs_exome_variants[key]) > 0:\n",
    "        final_variant_vcf[key] = combined_wgs_exome_variants[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix overlapping variants within each gene + apply SIFT/PolyPhen and VAF filters\n",
    "| Position |   V1   |   V2   |   V3   |   V4   |   V5   |   V6   |   V7   |   V8   |   V9   |  V10   |  V11   |\n",
    "| :------- | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n",
    "| Start (0-based)    | 126244 | 126244 | 126253 | 126255 | 126258 | 126258 | 126260 | 126264 | 126264 | 126265 | **126279** |\n",
    "| End (1-based)      | 126245 | 126254 | 126254 | 126256 | 126259 | 126262 | **126279** | 126265 | 126265 | 126266 | 126280 |\n",
    "\n",
    "### Combine variant positions for:\n",
    "+ adjacent bases: **V7 ends at 126279 (1-based) and V11 starts at 126279 (0-based). The variant windows are combined.**\n",
    "+ equivalent variants (in terms of position) \n",
    "+ overlapping variants\n",
    "\n",
    "### The determined variant range should encompass all of the above variants\n",
    "| Position | window 1 | window 2 | window 3 | \n",
    "| :------: | :------: | :------: | :------: |\n",
    "| Start    | 126244   | 126255   | 126258   |\n",
    "| End      | 126254   | 126256   | 126280   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inititalize the master list \n",
    "unique_variant_pos_all_genes = defaultdict(list)\n",
    "for key in final_variant_vcf.keys():\n",
    "    subset_variants = final_variant_vcf[key]\n",
    "    ## Get all of the start and stop variant positions within the gene\n",
    "    start_positions = [d[\"chr_start\"] for d in subset_variants]\n",
    "    end_positions = [d[\"chr_stop\"] for d in subset_variants]\n",
    "    exonic_end_positions = [d[\"stop\"] for d in subset_variants]\n",
    "\n",
    "    ## Go throouh each start/end position and check if there are overlapping variants \n",
    "    unique_variant_pos_per_gene = []\n",
    "    positions = []\n",
    "    for start, end in zip(start_positions, end_positions):\n",
    "        ## Get variants that have the same start/stop position \n",
    "        temp1 = list(filter(lambda subset_variants: \n",
    "               subset_variants[\"chr_start\"] <= start and subset_variants[\"chr_stop\"] >= start, subset_variants))\n",
    "        ## Check to see if there are variants that overlap with the end position\n",
    "        new_end = max([d[\"chr_stop\"] for d in temp1])\n",
    "        temp2 = list(filter(lambda subset_variants:\n",
    "                   subset_variants[\"chr_start\"] <= new_end and subset_variants[\"chr_stop\"] >= new_end, subset_variants))\n",
    "        ## Combine all variants that overlap with the intial variant in question\n",
    "        for v in temp2: \n",
    "            if v not in temp1: \n",
    "                temp1.append(v)\n",
    "\n",
    "        ## Get the index at which an adjacent variant's start position overlaps wtih the end position of the adjusted window\n",
    "        ## Assign the maximum end pos of the adjacent variant as the window's stop position\n",
    "        while max(set([d[\"chr_stop\"] for d in temp1])) in start_positions and \\\n",
    "        max(set([d[\"chr_stop\"] for d in temp1])) not in positions: ## Make sure it does not pull a variant that was already accounted for in a window\n",
    "            d = defaultdict(list)\n",
    "            for index, e in enumerate(start_positions):\n",
    "                d[e].append(index)\n",
    "            ## Get all the end positions of adjacent variants \n",
    "            end_index = d[max(set([d[\"chr_stop\"] for d in temp1]))]\n",
    "            ## Replace the maximum of the adjusted window with the adjacent variant's stop\n",
    "            temp1[len(temp1)-1][\"stop\"] = max(list(np.array(exonic_end_positions)[end_index]))\n",
    "            temp1[len(temp1)-1][\"chr_stop\"] = max(list(np.array(end_positions)[end_index]))\n",
    "\n",
    "        ## If the variant is NOT the first variant \n",
    "        if len(positions) > 0: \n",
    "            ## Check to see if the variant's start and stop falls in a window already calculated \n",
    "            prev_start_positions = [d[\"final_start_positions\"] for d in positions]\n",
    "            prev_end_positions = [d[\"final_end_positions\"] for d in positions]\n",
    "            for s, e in zip(prev_start_positions, prev_end_positions):\n",
    "                ## If the variant' start or end falls within a previously calculated window, skip the variant \n",
    "                if (start >= s and start <= e) or (end >= s and start <= e):\n",
    "                    temp1 = []\n",
    "                    break\n",
    "\n",
    "        ## If there are overlapping variants\n",
    "        if len(temp1) > 1: \n",
    "            final_temp = {\n",
    "                \"gene\": key,\n",
    "                \"start\": min(set([d[\"start\"] for d in temp1])),\n",
    "                \"stop\": max(set([d[\"stop\"] for d in temp1])),\n",
    "                \"strand\": temp1[1][\"strand\"],\n",
    "                \"exon\": temp1[1][\"exon\"],\n",
    "                \"chr\": temp1[1][\"chr\"],\n",
    "                \"chr_start\": min(set([d[\"chr_start\"] for d in temp1])),\n",
    "                \"chr_stop\": max(set([d[\"chr_stop\"] for d in temp1])),\n",
    "                \"mutation\": \"overlapping mutations\",\n",
    "                \"vaf\": temp1[1][\"vaf\"],\n",
    "                \"SIFT\": temp1[1][\"SIFT\"],\n",
    "                \"PolyPhen\": temp1[1][\"PolyPhen\"]\n",
    "            }\n",
    "            unique_variant_pos_per_gene.append(final_temp)\n",
    "\n",
    "        ## If there are no overlapping variants \n",
    "        if len(temp1) == 1:\n",
    "            final_temp = temp1[0]\n",
    "            unique_variant_pos_per_gene.append(final_temp)\n",
    "\n",
    "        ## Get the complete start and stop positions for variants within gene\n",
    "        test = {\n",
    "        \"final_start_positions\": final_temp[\"chr_start\"], ## 0-based\n",
    "        \"final_end_positions\": final_temp[\"chr_stop\"] ## 1-based \n",
    "        } \n",
    "\n",
    "        ## Store the start and stop positions for each adjusted variant range\n",
    "        if len(temp1) > 0:\n",
    "            positions.append(test)\n",
    "    ## Sort each variant within each gene based on the genomic start position\n",
    "    unique_variant_pos_per_gene = sorted(unique_variant_pos_per_gene, key = lambda i: i[\"chr_start\"])\n",
    "    unique_variant_pos_all_genes[key] = unique_variant_pos_per_gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_variant_pos_all_genes[test_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caclulate CCR window sizes and positions based on variant coordinates\n",
    "```\n",
    "   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19  20    ## 1 base\n",
    " | T | T | G | A | T | C | G | A | T | A | G | C | T | A | C | C | G | A | T | C |\n",
    " 0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19  20  ## 0 base \n",
    "```\n",
    "**Examples**\n",
    "+ **1 base b/w variants:** 2 *(1-based; variant_plus_one.start; 2-3 G)* - 1 *(0-based; variant.end; 0-1 T)* = 1\n",
    "+ **Multiple bases b/w variants**: 10 *(1-based; variant_plus_one.start; 10-11 G)* - 3 *(0-based; variant.end; 0-4 TTGA)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store the genes as keys \n",
    "## Store the window sizes/variant positions in dictionaries  \n",
    "final_window_dataset = {}\n",
    "for key in unique_variant_pos_all_genes.keys(): \n",
    "    ## Subset dictionaries (i.e. list of variants) based on each key (i.e. each gene)\n",
    "    subset_variants = unique_variant_pos_all_genes[key]\n",
    "    ## Go through the variants within each gene and measure the CCR window size \n",
    "    subset_window_list = []\n",
    "    ## Go through each variant, get the variant pair, and then calculate window size \n",
    "    for variant, variant_plus_one in zip(subset_variants, subset_variants[1:]):\n",
    "        ## Calculate window position of the variant\n",
    "        variant1_window = {\"gene\": key,\n",
    "        \"chromosome\": \"chr\" + variant[\"chr\"],\n",
    "        \"variant1_start\": variant[\"start\"],\n",
    "        \"variant1_stop\": variant[\"stop\"],\n",
    "        \"variant2_start\": \"NA\",\n",
    "        \"variant2_stop\": \"NA\",\n",
    "        \"variant1_vaf\": variant[\"vaf\"],\n",
    "        \"variant2_vaf\": \"NA\",\n",
    "        \"variant1_mut\": variant[\"mutation\"],\n",
    "        \"variant2_mut\": \"NA\",\n",
    "        \"variant1_SIFT\": variant[\"SIFT\"],\n",
    "        \"variant1_PolyPhen\": variant[\"PolyPhen\"],\n",
    "        \"variant2_SIFT\": \"NA\",\n",
    "        \"variant2_PolyPhen\": \"NA\",\n",
    "        \"window_size\": 0,\n",
    "        \"window_start\": variant[\"chr_start\"],\n",
    "        \"window_stop\": variant[\"chr_stop\"]\n",
    "        }\n",
    "\n",
    "        ## Calculate window positions between variants\n",
    "        window_dict = {\"gene\": key,\n",
    "        \"chromosome\": \"chr\" + variant[\"chr\"],\n",
    "        \"variant1_start\": variant[\"start\"],\n",
    "        \"variant1_stop\": variant[\"stop\"],\n",
    "        \"variant2_start\": variant_plus_one[\"start\"],\n",
    "        \"variant2_stop\": variant_plus_one[\"stop\"],\n",
    "        \"variant1_vaf\": variant[\"vaf\"],\n",
    "        \"variant2_vaf\": variant_plus_one[\"vaf\"],\n",
    "        \"variant1_mut\": variant[\"mutation\"],\n",
    "        \"variant2_mut\": variant_plus_one[\"mutation\"],\n",
    "        \"variant1_SIFT\": variant[\"SIFT\"],\n",
    "        \"variant1_PolyPhen\": variant[\"PolyPhen\"],\n",
    "        \"variant2_SIFT\": variant_plus_one[\"SIFT\"],\n",
    "        \"variant2_PolyPhen\": variant_plus_one[\"PolyPhen\"],\n",
    "        \"window_size\": max(0, variant_plus_one[\"start\"] - variant[\"stop\"]),\n",
    "        \"window_start\": variant[\"chr_stop\"],\n",
    "        \"window_stop\": variant_plus_one[\"chr_start\"]\n",
    "        }\n",
    "\n",
    "        ## Append window for variant and adjacent variants\n",
    "        subset_window_list.append(variant1_window)\n",
    "        if window_dict[\"window_size\"] > 0 :\n",
    "            subset_window_list.append(window_dict)\n",
    "\n",
    "    ## Get the information for the last variant in the GOI \n",
    "    last_variant = subset_variants[len(subset_variants)-1]\n",
    "    last_variant = {\"gene\": key,\n",
    "        \"chromosome\": \"chr\" + last_variant[\"chr\"],\n",
    "        \"variant1_start\": last_variant[\"start\"],\n",
    "        \"variant1_stop\": last_variant[\"stop\"],\n",
    "        \"variant2_start\": \"NA\",\n",
    "        \"variant2_stop\": \"NA\",\n",
    "        \"variant1_vaf\": last_variant[\"vaf\"],\n",
    "        \"variant2_vaf\": \"NA\",\n",
    "        \"variant1_mut\": last_variant[\"mutation\"],\n",
    "        \"variant2_mut\": \"NA\",\n",
    "        \"variant1_SIFT\": last_variant[\"SIFT\"],\n",
    "        \"variant1_PolyPhen\": last_variant[\"PolyPhen\"],\n",
    "        \"variant2_SIFT\": \"NA\",\n",
    "        \"variant2_PolyPhen\": \"NA\",\n",
    "        \"window_size\": 0,\n",
    "        \"window_start\": last_variant[\"chr_start\"],\n",
    "        \"window_stop\": last_variant[\"chr_stop\"]\n",
    "        }\n",
    "    ## Append window for the last variant \n",
    "    subset_window_list.append(last_variant)\n",
    "\n",
    "    ## Append each gene's window data to the associated gene in the final dataset\n",
    "    assert(key not in final_window_dataset)\n",
    "    final_window_dataset[key] = subset_window_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_window_dataset[test_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the percentiles for each CCR window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the 'windows' list  \n",
    "windows = []\n",
    "## Go through each gene and get the window sizes\n",
    "for key in final_window_dataset.keys():\n",
    "    subset_variants = final_window_dataset[key]\n",
    "    for variant in subset_variants:\n",
    "        windows.append(variant[\"window_size\"])\n",
    "## Sort the 'windows' list\n",
    "windows.sort()\n",
    "## Convert windows to numpy array\n",
    "windows = np.array(windows)\n",
    "## Go through each of the variants and obtain the scores \n",
    "final_dataset = defaultdict(list)\n",
    "for key in final_window_dataset.keys():\n",
    "    ## Get the variants found within each gene\n",
    "    subset_variants = final_window_dataset[key]\n",
    "    ## Define a percentiles list with each window's percentile + other information as the dictionary\n",
    "    percentiles_single_gene = []\n",
    "    for variant in subset_variants:\n",
    "        ## Determine the percentile of each window size within a single gene \n",
    "        single_score = round(percentileofscore(windows, variant[\"window_size\"]), 2)\n",
    "        ## Add a new key to the variant's dictionary \n",
    "        variant[\"percentile\"] = single_score\n",
    "        ## Append the dictionary to the list of percentiles for each gene\n",
    "        percentiles_single_gene.append(variant)\n",
    "    ## Appened all percentiles across all genes to the final dataset list \n",
    "    final_dataset[key] = percentiles_single_gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset[test_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a BED file to run in PathoScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use panda dataframes to make BED files for PathoScore\n",
    "new_CCR_bed = pd.DataFrame()\n",
    "for key in final_dataset.keys():\n",
    "    gene_pandaDf = pd.DataFrame(final_dataset[key], columns=[\"chromosome\", \"window_start\", \n",
    "        \"window_stop\", \"gene\", \n",
    "        \"window_size\", \"percentile\"])\n",
    "    new_CCR_bed = pd.concat([new_CCR_bed, gene_pandaDf])\n",
    "## Remove windows that are 0 in length \n",
    "# new_CCR_bed = new_CCR_bed[new_CCR_bed[\"window_size\"] > 0]\n",
    "## Change the column names of the combined panda dataframes and remove window_size column\n",
    "new_CCR_bed.columns = [\"#chrom\", \"start\", \"end\", \"gene\", \"window_size\", \"ccr_pct\"]\n",
    "# new_CCR_bed = new_CCR_bed.drop(columns=\"window_size\")\n",
    "\n",
    "## Sort the panda dataframe \n",
    "new_CCR_bed = new_CCR_bed.sort_values(by = [\"#chrom\", \"start\", \"end\"])\n",
    "\n",
    "## Write the bed file as a tab delimited txt file \n",
    "output_file = pathoscore_name + \".bed\" \n",
    "new_CCR_bed.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "new_CCR_bed.to_csv((pathoscore_name + \".txt\"), sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_CCR_bed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the script needed to run the PathoScore function on all bed files present in the directory \n",
    "\n",
    "Step 1: create the scripts needed to run PathoScore \n",
    "\n",
    "Step 2: bgzip and tabix all of the bed files present in the directory (TODO)\n",
    "\n",
    "Step 3: annotate benign and pathogenic variants from all bed files \n",
    "\n",
    "Step 4: run the PathoScore evaluate function on annotated benign and pathogenic files \n",
    "\n",
    "## Write the PathoScore scripts based on the bed files present in the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_files = os.listdir(\".\")\n",
    "pattern = \"*.bed\"\n",
    "## Create a list of bed files there associated column numbers \n",
    "bedfile_colnum_list = []\n",
    "for file in list_of_files: \n",
    "    if fnmatch.fnmatch(file, pattern):\n",
    "        reader = csv.reader(open(file, 'r'), delimiter='\\t')\n",
    "        bedfile_colnum_list.append({\n",
    "            \"filename\": file,\n",
    "            \"filename_no_suffix\": file.split(\".bed\")[0],\n",
    "            \"colnum\": len(next(reader))\n",
    "        })\n",
    "\n",
    "## Use the \n",
    "score_script = []\n",
    "evaluate_script = []\n",
    "for i in range(len(bedfile_colnum_list)): \n",
    "    filename = bedfile_colnum_list[i][\"filename\"]\n",
    "    filename_no_suffix = bedfile_colnum_list[i][\"filename_no_suffix\"]\n",
    "    colnum = str(bedfile_colnum_list[i][\"colnum\"])\n",
    "    score_script.append(\"--scores \" + filename + \"/\" + filename_no_suffix + \"/\" + colnum + \"/max\")\n",
    "    evaluate_script.append(\"-s \" + filename_no_suffix)\n",
    "separator = \" \"\n",
    "benign_score_script = \"python pathoscore.py annotate --prefix benign \" + separator.join(score_script) + \"pathoscore/truth-sets/GRCh37/clinvar/clinvar-benign.20170905.vcf.gz\"\n",
    "pathogenic_score_script = \"python pathoscore.py annotate --prefix pathogenic \" + separator.join(score_script) + \"pathoscore/truth-sets/GRCh37/clinvar/clinvar-pathogenic.20170905.vcf.gz\"\n",
    "pathoscore_evaluate_script = \"python pathoscore.py evaluate \" + separator.join(evaluate_script) + \" pathogenic.vcf.gz benign.vcf.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(benign_score_script)\n",
    "print(pathogenic_score_script)\n",
    "print(pathoscore_evaluate_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = open(r\"pathoscore_scripts.sh\", \"w+\")\n",
    "file_object.write(\"%s\\n%s\\n%s\\n\" % (benign_score_script, pathogenic_score_script, pathoscore_evaluate_script))\n",
    "file_object.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bgzip all of the bed files in the directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "bash /Users/jasonkunisaki/git/quinlan_lab_rotation/scripts/bgzip.sh /Users/jasonkunisaki/git/quinlan_lab_rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the PathoScore funtions that were generated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "bash /Users/jasonkunisaki/git/quinlan_lab_rotation/pathoscore_scripts.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move PathoScore output files to the appropriate output directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "bash /Users/jasonkunisaki/git/quinlan_lab_rotation/scripts/move_files.sh /Users/jasonkunisaki/git/quinlan_lab_rotation/pathoscore_results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print ROC curve from the PathoScore results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG('/Users/jasonkunisaki/git/quinlan_lab_rotation/pathoscore_results/pathoscore.roc.svg') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
